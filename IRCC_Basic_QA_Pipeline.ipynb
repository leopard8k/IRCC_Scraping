{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "IRCC_Basic_QA_Pipeline.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leopard8k/IRCC_Scraping/blob/master/IRCC_Basic_QA_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C03tlPO6uCZ_"
      },
      "source": [
        "!pip install html2text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMhtCoqGm8aV"
      },
      "source": [
        "import requests\r\n",
        "import urllib.request\r\n",
        "import time\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import re\r\n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOQhNyN3nOY6"
      },
      "source": [
        "CND_SITE = 'https://www.canada.ca'\r\n",
        "IRCC_SUFFIX='/en/immigration-refugees-citizenship/'\r\n",
        "filter = re.compile('^' + IRCC_SUFFIX + '.*')\r\n",
        "exclude_some = re.compile(\"#\")\r\n",
        "url = CND_SITE + IRCC_SUFFIX\r\n",
        "scraped_uris={\r\n",
        "              IRCC_SUFFIX: {\r\n",
        "                  \"url\": CND_SITE + IRCC_SUFFIX,\r\n",
        "               \"broken\": False,\r\n",
        "               \"visited\": False,\r\n",
        "               \"children\": {},\r\n",
        "               \"text\":\"\",\r\n",
        "               }\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiXAAqhB1chw"
      },
      "source": [
        "def get_dict_hrefs(uri):\r\n",
        "  for link in tqdm.tqdm(uri.copy()):\r\n",
        "    if not uri[link].get(\"visited\"):\r\n",
        "      uri[link][\"visited\"] = True\r\n",
        "      response = requests.get(CND_SITE+link)\r\n",
        "      if response.status_code == 200:\r\n",
        "        uri[link][\"broken\"] = False\r\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\r\n",
        "        ##uri[link][\"children\"] = set(sorted([a['href'] for a in soup.findAll('a', href=filter) if not exclude_some.search(a['href'])]))\r\n",
        "        ## uri[link][\"text\"] = soup\r\n",
        "        ##uri.update({a:{} for a in uri[link][\"children\"] if a not in uri})\r\n",
        "        uri.update({a['href']:{} for a in soup.findAll('a', href=filter) if not exclude_some.search(a['href']) and a['href'] not in uri}) \r\n",
        "      else:\r\n",
        "        uri[link][\"broken\"] = True      \r\n",
        "  \r\n",
        "  return uri"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBtTIskUpsyy"
      },
      "source": [
        "len1=len(scraped_uris)\r\n",
        "len1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCQlKLjTNF_N"
      },
      "source": [
        "### Repeat the following until no new members are added"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZdSk7SMy7mQ"
      },
      "source": [
        "len2 = len1 + 1\r\n",
        "while len2 > len1:\r\n",
        "  len1 = len2\r\n",
        "  scraped_uris.update(get_dict_hrefs(scraped_uris))\r\n",
        "  len2 = len(scraped_uris)\r\n",
        "len2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d6LfR--nlXf"
      },
      "source": [
        "count = 10000\r\n",
        "#with tqdm.tqdm(scraped_uris, desc=\"Downloading\") as uris:\r\n",
        "for link in tqdm.tqdm(scraped_uris, desc=\"Downloading\"):\r\n",
        "  download_url = CND_SITE + link\r\n",
        "  response = requests.get(download_url)\r\n",
        "  if response.status_code == 200:\r\n",
        "    count += 1\r\n",
        "    with open('./file-'+str(count)+'.html.txt', \"w\") as outfile:\r\n",
        "      outfile.write(h2t.handle(response.text))\r\n",
        "    #urllib.request.urlretrieve(download_url,'./file-'+str(count)+'.html') \r\n",
        "    time.sleep(0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrvJAGIm167b"
      },
      "source": [
        "!cat file-16355.html.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1DulZKD3W1W"
      },
      "source": [
        "import html2text\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZtV001H3dGY"
      },
      "source": [
        "h2t = html2text.HTML2Text()\r\n",
        "h2t.ignore_images = True\r\n",
        "h2t.ignore_links = True\r\n",
        "h2t.ignore_emphasis = True\r\n",
        "h2t.ignore_tables = True\r\n",
        "h2t.handle('<p>this is some html </p><td></td><a href=\"https://google.com\"> some link text</a>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVtz5EG8txpB"
      },
      "source": [
        "!mkdir dataIRCC\r\n",
        "!for file in file*html;do html2text ${file} > dataIRCC/${file}.txt;done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UtYUeXYOleU9"
      },
      "source": [
        "# Make sure you have a GPU running\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mckDJzOleU_"
      },
      "source": [
        "# Install the latest release of Haystack in your own environment \n",
        "#! pip install farm-haystack\n",
        "\n",
        "# Install the latest master of Haystack\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git\n",
        "!pip install urllib3==1.25.4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IKhcy99leVA"
      },
      "source": [
        "from haystack.preprocessor.cleaning import clean_wiki_text\n",
        "from haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http\n",
        "from haystack.reader.farm import FARMReader\n",
        "from haystack.reader.transformers import TransformersReader\n",
        "from haystack.utils import print_answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILd3GtpHleVB"
      },
      "source": [
        "## Document Store\n",
        "\n",
        "Haystack finds answers to queries within the documents stored in a `DocumentStore`. The current implementations of `DocumentStore` include `ElasticsearchDocumentStore`, `FAISSDocumentStore`,  `SQLDocumentStore`, and `InMemoryDocumentStore`.\n",
        "\n",
        "**Here:** We recommended Elasticsearch as it comes preloaded with features like [full-text queries](https://www.elastic.co/guide/en/elasticsearch/reference/current/full-text-queries.html), [BM25 retrieval](https://www.elastic.co/elasticon/conf/2016/sf/improved-text-scoring-with-bm25), and [vector storage for text embeddings](https://www.elastic.co/guide/en/elasticsearch/reference/7.6/dense-vector.html).\n",
        "\n",
        "**Alternatives:** If you are unable to setup an Elasticsearch instance, then follow the [Tutorial 3](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb) for using SQL/InMemory document stores.\n",
        "\n",
        "**Hint**: This tutorial creates a new document store instance with Wikipedia articles on Game of Thrones. However, you can configure Haystack to work with your existing document stores.\n",
        "\n",
        "### Start an Elasticsearch server\n",
        "You can start Elasticsearch on your local machine instance using Docker. If Docker is not readily available in your environment (eg., in Colab notebooks), then you can manually download and execute Elasticsearch from source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCfVbeSOleVC"
      },
      "source": [
        "# Recommended: Start Elasticsearch using Docker\n",
        "#! docker run -d -p 9200:9200 -e \"discovery.type=single-node\" elasticsearch:7.9.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1L3zFomleVF"
      },
      "source": [
        "# In Colab / No Docker environments: Start Elasticsearch from source\n",
        "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "! tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "! chown -R daemon:daemon elasticsearch-7.9.2\n",
        "\n",
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['elasticsearch-7.9.2/bin/elasticsearch'],\n",
        "                   stdout=PIPE, stderr=STDOUT,\n",
        "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                  )\n",
        "# wait until ES has started\n",
        "! sleep 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2JzRRGO-leVG"
      },
      "source": [
        "# Connect to Elasticsearch\n",
        "\n",
        "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
        "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "lxIMemRXleVJ"
      },
      "source": [
        "## Preprocessing of documents\n",
        "\n",
        "Haystack provides a customizable pipeline for:\n",
        " - converting files into texts\n",
        " - cleaning texts\n",
        " - splitting texts\n",
        " - writing them to a Document Store\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Asmqw4swleVK"
      },
      "source": [
        "\n",
        "doc_dir = \"dataIRCC\"\n",
        "\n",
        "# Convert files to dicts\n",
        "# You can optionally supply a cleaning function that is applied to each doc (e.g. to remove footers)\n",
        "# It must take a str as input, and return a str.\n",
        "dicts = convert_files_to_dicts(dir_path=doc_dir, split_paragraphs=False)\n",
        "\n",
        "# We now have a list of dictionaries that we can write to our document store.\n",
        "# If your texts come from a different source (e.g. a DB), you can of course skip convert_files_to_dicts() and create the dictionaries yourself.\n",
        "# The default format here is:\n",
        "# {\n",
        "#    'text': \"<DOCUMENT_TEXT_HERE>\",\n",
        "#    'meta': {'name': \"<DOCUMENT_NAME_HERE>\", ...}\n",
        "#}\n",
        "# (Optionally: you can also add more key-value-pairs here, that will be indexed as fields in Elasticsearch and\n",
        "# can be accessed later for filtering or shown in the responses of the Finder)\n",
        "\n",
        "# Let's have a look at the first 3 entries:\n",
        "print(dicts[:3])\n",
        "\n",
        "# Now, let's write the dicts containing documents to our DB.\n",
        "document_store.write_documents(dicts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QkRFq0mleVM"
      },
      "source": [
        "## Initalize Retriever, Reader,  & Finder\n",
        "\n",
        "### Retriever\n",
        "\n",
        "Retrievers help narrowing down the scope for the Reader to smaller units of text where a given question could be answered.\n",
        "They use some simple but fast algorithm.\n",
        "\n",
        "**Here:** We use Elasticsearch's default BM25 algorithm\n",
        "\n",
        "**Alternatives:**\n",
        "\n",
        "- Customize the `ElasticsearchRetriever`with custom queries (e.g. boosting) and filters\n",
        "- Use `TfidfRetriever` in combination with a SQL or InMemory Document store for simple prototyping and debugging\n",
        "- Use `EmbeddingRetriever` to find candidate documents based on the similarity of embeddings (e.g. created via Sentence-BERT)\n",
        "- Use `DensePassageRetriever` to use different embedding models for passage and query (see Tutorial 6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc2oPgBQleVO"
      },
      "source": [
        "from haystack.retriever.sparse import ElasticsearchRetriever\n",
        "retriever = ElasticsearchRetriever(document_store=document_store)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "KaKfqEU_leVP"
      },
      "source": [
        "# Alternative: An in-memory TfidfRetriever based on Pandas dataframes for building quick-prototypes with SQLite document store.\n",
        "\n",
        "# from haystack.retriever.sparse import TfidfRetriever\n",
        "# retriever = TfidfRetriever(document_store=document_store)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPRGK8xyleVP"
      },
      "source": [
        "### Reader\n",
        "\n",
        "A Reader scans the texts returned by retrievers in detail and extracts the k best answers. They are based\n",
        "on powerful, but slower deep learning models.\n",
        "\n",
        "Haystack currently supports Readers based on the frameworks FARM and Transformers.\n",
        "With both you can either load a local model or one from Hugging Face's model hub (https://huggingface.co/models).\n",
        "\n",
        "**Here:** a medium sized RoBERTa QA model using a Reader based on FARM (https://huggingface.co/deepset/roberta-base-squad2)\n",
        "\n",
        "**Alternatives (Reader):** TransformersReader (leveraging the `pipeline` of the Transformers package)\n",
        "\n",
        "**Alternatives (Models):** e.g. \"distilbert-base-uncased-distilled-squad\" (fast) or \"deepset/bert-large-uncased-whole-word-masking-squad2\" (good accuracy)\n",
        "\n",
        "**Hint:** You can adjust the model to return \"no answer possible\" with the no_ans_boost. Higher values mean the model prefers \"no answer possible\"\n",
        "\n",
        "#### FARMReader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "aX5rLrylleVR"
      },
      "source": [
        "# Load a  local model or any of the QA models on\n",
        "# Hugging Face's model hub (https://huggingface.co/models)\n",
        "\n",
        "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JErUvzplleVS"
      },
      "source": [
        "#### TransformersReader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crMuaRhVleVT"
      },
      "source": [
        "# Alternative:\n",
        "# reader = TransformersReader(model_name_or_path=\"distilbert-base-uncased-distilled-squad\", tokenizer=\"distilbert-base-uncased\", use_gpu=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_--AupVwleVU"
      },
      "source": [
        "### Pipeline\n",
        "\n",
        "With a Haystack `Pipeline` you can stick together your building blocks to a search pipeline.\n",
        "Under the hood, `Pipelines` are Directed Acyclic Graphs (DAGs) that you can easily customize for your own use cases.\n",
        "To speed things up, Haystack also comes with a few predefined Pipelines. One of them is the `ExtractiveQAPipeline` that combines a retriever and a reader to answer our questions.\n",
        "You can learn more about `Pipelines` in the [docs](https://haystack.deepset.ai/docs/latest/pipelinesmd)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "W-c2m5o5leVV"
      },
      "source": [
        "from haystack.pipeline import ExtractiveQAPipeline\n",
        "pipe = ExtractiveQAPipeline(reader, retriever)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTa5lPg7leVV"
      },
      "source": [
        "## Voil√†! Ask a question!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "Vtsvlj2kleVW"
      },
      "source": [
        "# You can configure how many candidates the reader and retriever shall return\n",
        "# The higher top_k_retriever, the better (but also the slower) your answers. \n",
        "prediction = pipe.run(query=\"can people with covid19 come to canada?\", top_k_retriever=10, top_k_reader=5)\n",
        "print_answers(prediction, details=\"minimal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpWJKZozY0GK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}