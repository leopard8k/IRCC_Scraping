{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Copy of IRCC_scrape.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itmkSuvcsYpM"
      },
      "source": [
        "# Webscrape IRCC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsLhvdarsYpR"
      },
      "source": [
        "import requests\n",
        "import urllib.request\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import re"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkOA1-T9sYpS"
      },
      "source": [
        "### Set the URL you want to webscrape from"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkNw1ZyGsYpS"
      },
      "source": [
        "CND_SITE = 'https://www.canada.ca'\r\n",
        "IRCC_SUFFIX='/en/immigration-refugees-citizenship/'\r\n",
        "filter = re.compile('^' + IRCC_SUFFIX + '.*')\r\n",
        "exclude_some = re.compile(\"#\")\r\n",
        "url = CND_SITE + IRCC_SUFFIX"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ijy4quOlI_4M"
      },
      "source": [
        "def get_hrefs(url):\r\n",
        "  response = requests.get(url)\r\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\r\n",
        "  return set(sorted([a['href'] for a in soup.findAll('a', href=filter) if not exclude_some.search(a['href'])]))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqW2MSx6sYpT"
      },
      "source": [
        "### Connect to the site - get the first page and loop through links to get children\r\n",
        "> #### Paramateres\r\n",
        ">> #### SCRAPE_DEPTH: determines how many layers we loop through - less: quicker; more: more pages; setting it at 2 returns a bit over 5000 links; setting it too high make it take longer to complete\r\n",
        ">> #### WEBCALLS_WITH_NO_BREAK: determines how often we take a one-second break from scraping. It helps to prevent from flooding the site and risk our IP be banned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh3QjR87Jr28",
        "outputId": "2ce83c90-121a-40f6-84ae-e5738c16d263"
      },
      "source": [
        "scraped_uris = get_hrefs(url)\r\n",
        "# in my experience two iterations returns over 5000 links\r\n",
        "#   one can experience with more if necessary\r\n",
        "SCRAPE_DEPTH = 1\r\n",
        "WEBCALLS_WITH_NO_BREAK = 10\r\n",
        "\r\n",
        "for i in range(SCRAPE_DEPTH):\r\n",
        "  new_uris = set()\r\n",
        "  web_calls=0\r\n",
        "  for suffix in scraped_uris:\r\n",
        "    new_uris |= get_hrefs(CND_SITE+suffix)\r\n",
        "    # take a second between every few site calls to not flood the site\r\n",
        "    web_calls += 1\r\n",
        "    if web_calls >= WEBCALLS_WITH_NO_BREAK:\r\n",
        "      time.sleep(1)\r\n",
        "      web_calls = 0\r\n",
        "  scraped_uris |= new_uris\r\n",
        "\r\n",
        "len(scraped_uris)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "629"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plrren2CsYpc"
      },
      "source": [
        "## To download the whole data set, let's do a for loop through all a tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWJDGaTgsYpe"
      },
      "source": [
        "count = 10000\r\n",
        "for ahref in scraped_uris:\r\n",
        "  download_url = CND_SITE + ahref\r\n",
        "  count += 1\r\n",
        "  urllib.request.urlretrieve(download_url,'./file-'+str(count)) \r\n",
        "  time.sleep(1)\r\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zPM7a1wuRmX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}